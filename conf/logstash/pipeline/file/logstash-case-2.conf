# Sample Logstash configuration for creating a simple
# CSV -> Logstash -> Elasticsearch pipeline.

# Input all csv in logs
input {
  file {
    path => ["/usr/share/logstash/logs/input/test-case-2.csv"]
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  csv {
    separator => ","
    skip_header => "true"
    columns => [
      "time",
      "c1",
      "c2",
      "c3"
    ]
    convert => {
      "time" => "date"
      "c1" => "integer"
      "c2" => "integer"
      "c3" => "integer"
    }
  }
  grok {
    match => ["path","%{GREEDYDATA}/%{GREEDYDATA:filename}\.csv"]
  }
  date {
    match => [ "time", "UNIX_MS" ]
    target => "@timestamp"
  }
}

output {
  stdout {}
  elasticsearch {
    hosts => "elasticsearch:9200"
    index => "%{filename}-%{+YYYY.MM.dd}"
  }
}
